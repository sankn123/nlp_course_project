# -*- coding: utf-8 -*-
"""finetune_InLegalBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d4bwab1Ke5JPYh_nUHOuzxIlkO0Prvu2
"""



import numpy as np
import torch
import torch.nn as nn
import torchmetrics
import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"

#root="/export/home/nlp1/rachit/Inlegalbert"

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("law-ai/CustomInLawBERT")
# text = "Replace this string with yours"
# encoded_input = tokenizer(text, return_tensors="pt")
Inlegal_model = AutoModel.from_pretrained("law-ai/CustomInLawBERT")
# output = model(**encoded_input)
# last_hidden_state = output.last_hidden_state

# import json
# import numpy as np


# with open('/content/drive/MyDrive/nlp course/legaldatatrain.json', 'r') as f:
#     data = json.load(f)


# texts = []
# labels = []


# for item in data:
#     annotations = item.get('annotations', [])
#     for annotation in annotations:
#         result = annotation.get('result', [])
#         for r in result:
#             text = r.get('value', {}).get('text', '')
#             label = r.get('value', {}).get('labels', [])
#             if text and label:
#                 texts.append(text)
#                 labels.append(label[0])


# texts_array = np.array(texts)
# labels_array = np.array(labels)

# texts_array.shape,labels_array.shape

# np.unique(labels_array)

# label_mapping = {
#     'ANALYSIS': 0,
#     'ARG_PETITIONER': 1,
#     'ARG_RESPONDENT': 2,
#     'FAC': 3,
#     'ISSUE': 4,
#     'NONE': 5,
#     'PREAMBLE': 6,
#     'PRE_NOT_RELIED': 7,
#     'PRE_RELIED': 8,
#     'RATIO': 9,
#     'RLC': 10,
#     'RPC': 11,
#     'STA': 12
# }


# labels_int = np.array([label_mapping[label] for label in labels_array])

# np.save('/content/drive/MyDrive/nlp course/labels_train.npy', labels_int)
# np.save('/content/drive/MyDrive/nlp course/texts_train.npy', texts_array)

X_train=np.load(f'texts_train.npy')
X_test=np.load(f'texts_dev.npy')
y_train=np.load(f'labels_train.npy')
y_test=np.load(f'labels_dev.npy')

print("Length of X_train is ", len(X_train))
print("Length of X_test is ", len(X_test))

print("Length of y_train is ", len(y_train))
print("Length of y_test is ", len(y_test))

print(np.unique(y_train))
tokenized_text_train=[]
tokenized_text_test=[]
for text in X_train:
  encoded_input = tokenizer.encode_plus(text,max_length=32,padding='max_length', truncation=True,return_tensors="pt")
  tokenized_text_train.append(encoded_input)
for text in X_test:
  encoded_input = tokenizer.encode_plus(text,max_length=32,padding='max_length',truncation=True, return_tensors="pt")
  tokenized_text_test.append(encoded_input)

num_classes=13
classifier=nn.Sequential(
    nn.Linear(768,512),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(512,128),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(128,num_classes),
    nn.Softmax()
)

for param in Inlegal_model.parameters():
    param.requires_grad=False
for p in classifier.parameters():
    p.requires_grad=True

class custom(nn.Module):
  def __init__(self):
    super().__init__()
    self.m1=Inlegal_model
    self.m2=classifier
  def forward(self,inp,tti,mask):
    x=self.m1(input_ids=inp,token_type_ids=tti,attention_mask=mask).pooler_output
    x=torch.squeeze(x, 0)
    x=self.m2(x)

    return x

model=custom()

inp_train=[]
masks_train=[]
tti_train=[]
for data in tokenized_text_train:
  inp=data['input_ids']
  token_type_id=data['token_type_ids']
  mask=data['attention_mask']

  inp_train.append(inp)
  masks_train.append(mask)
  tti_train.append(token_type_id)

inp_test=[]
masks_test=[]
tti_test=[]
for data in tokenized_text_test:
  inp=data['input_ids']
  token_type_id=data['token_type_ids']
  mask=data['attention_mask']

  inp_test.append(inp)
  masks_test.append(mask)
  tti_test.append(token_type_id)

inp_train=torch.stack(inp_train)
masks_train=torch.stack(masks_train)
tti_train=torch.stack(tti_train)

inp_test=torch.stack(inp_test)
masks_test=torch.stack(masks_test)
tti_test=torch.stack(tti_test)

y_train=torch.LongTensor(y_train)
y_test=torch.LongTensor(y_test)
y_train=y_train.to('cuda')
y_test=y_test.to('cuda')

inp_train=inp_train.to('cuda')
masks_train=masks_train.to('cuda')
tti_train=tti_train.to('cuda')

inp_test=inp_test.to('cuda')
masks_test=masks_test.to('cuda')
tti_test=tti_test.to('cuda')

model=model.to("cuda")

from torch.utils.data import DataLoader,TensorDataset

train_dataset=TensorDataset(inp_train,tti_train,masks_train,y_train)
val_dataset=TensorDataset(inp_test,tti_test,masks_test,y_test)


train = DataLoader(train_dataset, batch_size=64, shuffle=True)
val=DataLoader(val_dataset, batch_size=64, shuffle=True)

epochs=5
print(len(val),len(train))
criterion=nn.CrossEntropyLoss()
opt=torch.optim.Adam(model.parameters(), lr=0.1)
accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes).to('cuda')

for epoch in range(epochs):
  avg_val_acc=0
  count=0
  avg_train_loss=0
  avg_val_loss=0


  for batch in train:
    count=count+1
    if count%100==0:
        print(f'Epoch {epoch} Batch no.: {count}')
    X_batch_in,X_batch_tti,X_batch_mask,label_batch = batch

    X_batch_in=torch.squeeze(X_batch_in)
    X_batch_tti=torch.squeeze(X_batch_tti)
    X_batch_mask=torch.squeeze(X_batch_mask)

    preds=model(X_batch_in,X_batch_tti,X_batch_mask)

    loss=criterion(preds,label_batch)
    avg_train_loss=avg_train_loss+loss

    opt.zero_grad()
    loss.backward()
    opt.step()


  with torch.no_grad():
    for batch in val:
      X_val_batch_in,X_val_batch_tti,X_val_batch_mask,label_val_batch = batch

      X_val_batch_in=torch.squeeze(X_val_batch_in)
      X_val_batch_tti=torch.squeeze(X_val_batch_tti)
      X_val_batch_mask=torch.squeeze(X_val_batch_mask)

      val_preds=model(X_val_batch_in,X_val_batch_tti,X_val_batch_mask)
      val_loss=criterion(val_preds,label_val_batch)

      val_acc=accuracy.update(val_preds,label_val_batch)
      print(val_preds)
      print(label_val_batch)
      avg_val_loss=avg_val_loss+val_loss

  #print(f'Epoch {epoch}: Training Loss:  Training accuracy: , Validation Loss: Validation accuracy:')
  if epoch%1==0:
    print(f"| Epoch={epoch} |  Validation Accuracy={accuracy.compute()} | Training Loss={avg_train_loss/len(train)} | Validation_Loss={avg_val_loss/len(val)} |")
    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')

torch.save(model,f'weights/inlegalbert_{epochs}epochs.pt')

