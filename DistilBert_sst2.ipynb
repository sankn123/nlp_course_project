{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "Q-rnSxl-UoFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "oEvN20TwTjiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"sst2\")\n",
        "\n",
        "#  you can use any of the following config names as a second argument:\n",
        "#  \"ax\", \"cola\", \"mnli\", \"mnli_matched\",\n",
        "#  \"mnli_mismatched\", \"mrpc\", \"qnli\", \"qqp\",\n",
        "#  \"rte\", \"sst2\", \"stsb\", \"wnli\""
      ],
      "metadata": {
        "id": "u2zSAOLGUUFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "2fNGZ1jaXuX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "dbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "TauV8MFluKCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data prep ##\n",
        "#1-Train#\n",
        "seq_len=128\n",
        "train_x_in=[]\n",
        "train_x_mask=[]\n",
        "train_y=[]\n",
        "for i in range(dataset['train'].num_rows):\n",
        "\n",
        "  temp=dataset['train'][i]\n",
        "  sent=temp[\"sentence\"]\n",
        "  label=temp['label']\n",
        "  tok=tokenizer(sent, return_tensors='pt')\n",
        "  #token tensor and mask\n",
        "  input=tok.input_ids\n",
        "  mask=tok.attention_mask\n",
        "\n",
        "  if len(input[0])<=seq_len:\n",
        "    z=torch.zeros(seq_len-len(input[0]))\n",
        "    # print(z.shape)\n",
        "\n",
        "    input=torch.cat((input[0],z))\n",
        "    mask=torch.cat((mask[0],z))\n",
        "    # print(input.shape,mask.shape)\n",
        "\n",
        "    train_x_in.append(input)\n",
        "    train_x_mask.append(mask)\n",
        "\n",
        "    train_y.append(label)\n",
        "\n",
        "\n",
        "train_x_in=torch.stack(train_x_in)\n",
        "train_x_in=train_x_in.to(torch.long)\n",
        "\n",
        "# train_x_in=torch.LongTensor(train_x_in)\n",
        "\n",
        "train_x_mask=torch.stack(train_x_mask)\n",
        "train_x_mask=train_x_mask.to(torch.long)\n",
        "\n",
        "# train_x_mask=torch.LongTensor(train_x_mask)\n",
        "\n",
        "train_y=torch.LongTensor(train_y)\n",
        "\n",
        "\n",
        "#2-val#\n",
        "val_x_in=[]\n",
        "val_x_mask=[]\n",
        "val_y=[]\n",
        "for i in range(dataset['validation'].num_rows):\n",
        "\n",
        "  temp=dataset['validation'][i]\n",
        "  sent=temp[\"sentence\"]\n",
        "  label=temp['label']\n",
        "  tok=tokenizer(sent, return_tensors='pt')\n",
        "  #token tensor and mask\n",
        "  input=tok.input_ids\n",
        "  mask=tok.attention_mask\n",
        "\n",
        "  if len(input[0])<=seq_len:\n",
        "    z=torch.zeros(seq_len-len(input[0]))\n",
        "\n",
        "\n",
        "    input=torch.cat((input[0],z))\n",
        "    mask=torch.cat((mask[0],z))\n",
        "\n",
        "\n",
        "\n",
        "    val_x_in.append(input)\n",
        "    val_x_mask.append(mask)\n",
        "    val_y.append(label)\n",
        "\n",
        "\n",
        "val_x_in=torch.stack(val_x_in)\n",
        "val_x_in=val_x_in.to(torch.long)\n",
        "# val_x_in=torch.LongTensor(val_x_in)\n",
        "\n",
        "val_x_mask=torch.stack(val_x_mask)\n",
        "val_x_mask=val_x_mask.to(torch.long)\n",
        "# val_x_mask=torch.LongTensor(val_x_mask)\n",
        "\n",
        "val_y=torch.LongTensor(val_y)\n",
        "\n",
        "\n",
        "#3-test#\n",
        "test_x_in=[]\n",
        "test_x_mask=[]\n",
        "test_y=[]\n",
        "for i in range(dataset['test'].num_rows):\n",
        "\n",
        "  temp=dataset['test'][i]\n",
        "  sent=temp[\"sentence\"]\n",
        "  label=temp['label']\n",
        "  tok=tokenizer(sent, return_tensors='pt')\n",
        "  #token tensor and mask\n",
        "  input=tok.input_ids\n",
        "  mask=tok.attention_mask\n",
        "  if len(input[0])<=seq_len:\n",
        "    z=torch.zeros(seq_len-len(input[0]))\n",
        "\n",
        "    input=torch.cat((input[0],z))\n",
        "    mask=torch.cat((mask[0],z))\n",
        "\n",
        "    test_x_in.append(input)\n",
        "    test_x_mask.append(mask)\n",
        "    test_y.append(label)\n",
        "\n",
        "\n",
        "test_x_in=torch.stack(test_x_in)\n",
        "test_x_in=test_x_in.to(torch.long)\n",
        "# test_x_in=torch.LongTensor(test_x_in)\n",
        "\n",
        "test_x_mask=torch.stack(test_x_mask)\n",
        "test_x_mask=test_x_mask.to(torch.long)\n",
        "# test_x_mask=torch.LongTensor(test_x_mask)\n",
        "\n",
        "test_y=torch.LongTensor(test_y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8LamkY9qthEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_in.dtype"
      ],
      "metadata": {
        "id": "hw6qryQrZiV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_model.parameters"
      ],
      "metadata": {
        "id": "x2Jxov3JToRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_model.config.dim"
      ],
      "metadata": {
        "id": "34XhsEu6pwVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=nn.Sequential(\n",
        "    nn.Linear(dbert_model.config.dim,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512,128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128,2),\n",
        "    nn.Softmax()\n",
        ")"
      ],
      "metadata": {
        "id": "U8ZhVv8sppfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalysis(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.m1=dbert_model\n",
        "    self.m2=classifier\n",
        "  def forward(self,input,mask):\n",
        "    x=self.m1(input,mask).last_hidden_state\n",
        "    x=torch.squeeze(x, 0)\n",
        "    # print(x.shape)\n",
        "    x=torch.mean(x, 1)\n",
        "    # print(x.shape)\n",
        "    x=self.m2(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "j5qxbmP3TuFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=SentimentAnalysis()"
      ],
      "metadata": {
        "id": "ZrBsLqfBsA4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "metadata": {
        "id": "ccHEmwWKsDFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_in=train_x_in.to('cuda')\n",
        "train_x_mask=train_x_mask.to('cuda')\n",
        "train_y=train_y.to('cuda')\n",
        "val_x_in=val_x_in.to('cuda')\n",
        "val_x_mask=val_x_mask.to('cuda')\n",
        "val_y=val_y.to('cuda')\n",
        "test_x_in=test_x_in.to('cuda')\n",
        "test_x_mask=test_x_mask.to('cuda')\n",
        "test_y=test_y.to('cuda')\n",
        "model=model.to('cuda')"
      ],
      "metadata": {
        "id": "9f_-ZKbBTy06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "train_dataset=TensorDataset(train_x_in,train_x_mask,train_y)\n",
        "val_dataset=TensorDataset(val_x_in,val_x_mask,val_y)\n",
        "test_dataset=TensorDataset(test_x_in,test_x_mask,test_y)\n",
        "\n",
        "\n",
        "\n",
        "train = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val=DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "NGkWJIAtTxgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "metadata": {
        "id": "sB8MgnWltd3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=5\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "opt=torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to('cuda')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  avg_train_acc=0\n",
        "  avg_val_acc=0\n",
        "  count=0\n",
        "  avg_train_loss=0\n",
        "  avg_val_loss=0\n",
        "  precision=0\n",
        "  recall=0\n",
        "  fscore=0\n",
        "\n",
        "  for batch in train:\n",
        "    count=count+1\n",
        "    print(f'Epoch {epoch} Batch no.: {count}')\n",
        "    X_batch_in,X_batch_mask,label_batch = batch\n",
        "    # print(X_batch.shape)\n",
        "    preds=model(X_batch_in,X_batch_mask)\n",
        "\n",
        "    loss=criterion(preds,label_batch)\n",
        "    acc=accuracy(preds,label_batch)\n",
        "    avg_train_acc=avg_train_acc+acc\n",
        "    avg_train_loss=avg_train_loss+loss\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val:\n",
        "      X_val_batch_in,X_val_batch_mask,label_val_batch = batch\n",
        "\n",
        "\n",
        "      val_preds=model(X_val_batch_in,X_val_batch_mask)\n",
        "      val_loss=criterion(val_preds,label_val_batch)\n",
        "      val_acc=accuracy(val_preds,label_val_batch)\n",
        "      avg_val_acc=avg_val_acc+val_acc\n",
        "      avg_val_loss=avg_val_loss+val_loss\n",
        "      precision1, recall1, fscore1, sup = sklearn.metrics.precision_recall_fscore_support(y_2, Predicted_Label_2, average='weighted')\n",
        "      precision=precision + precision1\n",
        "      recall=recall+recall1\n",
        "      fscore=fscore+fscore1\n",
        "\n",
        "\n",
        "\n",
        "  # print(f'Epoch {epoch}: Training Loss:  Training accuracy: , Validation Loss: Validation accuracy:')\n",
        "  # if epoch%5==0:\n",
        "  print(f\"| Epoch={epoch} | Training Accuracy={avg_train_acc/len(train)} | Validation Accuracy={avg_val_acc/len(val)} | Training Loss={avg_train_loss/len(train)} | Validation_Loss={avg_val_loss/len(val)} |\")\n",
        "  print(f\"P={precision/len(iterator)}, R={recall/len(iterator)}, F1={fscore/len(iterator)}\")\n",
        "  print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Test acc:\n",
        "with torch.no_grad():\n",
        "    for batch in test:\n",
        "      X_test_batch_in,X_test_batch_mask,label_test_batch = batch\n",
        "\n",
        "\n",
        "      test_preds=model(X_test_batch_in,X_test_batch_mask)\n",
        "      test_loss=criterion(test_preds,label_test_batch)\n",
        "      test_acc=accuracy(test_preds,label_test_batch)\n",
        "      avg_test_acc=avg_test_acc+test_acc\n",
        "      avg_test_loss=avg_test_loss+test_loss\n",
        "\n",
        "\n",
        "print(f'Test loss: {avg_test_loss/len(test)} | Test acc: {avg_test_acc/len(test)}')\n",
        "\n",
        "new_model=new_model.to('cpu')\n",
        "# torch.save(new_model,'/content/drive/MyDrive/NLPCourse/test_model.pt')\n"
      ],
      "metadata": {
        "id": "J6FvvapfsEkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for batch in test:\n",
        "      X_test_batch_in,X_test_batch_mask,label_test_batch = batch\n",
        "\n",
        "\n",
        "      test_preds=model(X_test_batch_in,X_test_batch_mask)\n",
        "      test_loss=criterion(test_preds,label_test_batch)\n",
        "      test_acc=accuracy(test_preds,label_test_batch)\n",
        "      avg_test_acc=avg_test_acc+test_acc\n",
        "      avg_test_loss=avg_test_loss+test_loss\n",
        "\n",
        "\n",
        "print(f'Test loss: {avg_test_loss/len(test)} | Test acc: {avg_test_acc/len(test)}')"
      ],
      "metadata": {
        "id": "RyKEClVjs7AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GbrRPfLs7NX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}