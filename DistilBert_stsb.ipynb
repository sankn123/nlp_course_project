{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "xm62PRcogq5O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QImG7VoQgkh8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"stsb\")\n",
        "\n",
        "#  you can use any of the following config names as a second argument:\n",
        "#  \"ax\", \"cola\", \"mnli\", \"mnli_matched\",\n",
        "#  \"mnli_mismatched\", \"mrpc\", \"qnli\", \"qqp\",\n",
        "#  \"rte\", \"sst2\", \"stsb\", \"wnli\"\n",
        "\n",
        "dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "dbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "3XL21z2bgwTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data prep ##\n",
        "#1-Train#\n",
        "seq_len=128\n",
        "\n",
        "train_x_in=[]\n",
        "train_x_mask=[]\n",
        "train_y=[]\n",
        "for i in range(dataset['train'].num_rows):\n",
        "\n",
        "  temp=dataset['train'][i]\n",
        "  sent1=temp[\"sentence1\"]\n",
        "  sent2=temp['sentence2']\n",
        "  label=temp['label']\n",
        "  tok1=tokenizer(sent1, return_tensors='pt')\n",
        "  tok2=tokenizer(sent2, return_tensors='pt')\n",
        "  #token tensor and mask\n",
        "  input1=tok1.input_ids\n",
        "  mask1=tok1.attention_mask\n",
        "\n",
        "  input2=tok2.input_ids\n",
        "  mask2=tok2.attention_mask\n",
        "\n",
        "  input=[]\n",
        "  mask=[]\n",
        "  input=torch.cat((input1,input2),1)\n",
        "  mask=torch.cat((mask1,mask2),1)\n",
        "\n",
        "  if len(input[0])<=seq_len:\n",
        "    z=torch.zeros(seq_len-len(input[0]))\n",
        "    # print(z.shape)\n",
        "\n",
        "    input=torch.cat((input[0],z))\n",
        "    mask=torch.cat((mask[0],z))\n",
        "    # print(input.shape,mask.shape)\n",
        "\n",
        "    train_x_in.append(input)\n",
        "    train_x_mask.append(mask)\n",
        "\n",
        "    train_y.append(label)\n",
        "\n",
        "\n",
        "\n",
        "train_x_in=torch.stack(train_x_in)\n",
        "train_x_in=train_x_in.to(torch.long)\n",
        "\n",
        "\n",
        "\n",
        "# train_x_in=torch.LongTensor(train_x_in)\n",
        "\n",
        "train_x_mask=torch.stack(train_x_mask)\n",
        "train_x_mask=train_x_mask.to(torch.long)\n",
        "\n",
        "\n",
        "# train_x_mask=torch.LongTensor(train_x_mask)\n",
        "\n",
        "train_y=torch.LongTensor(train_y)\n",
        "\n",
        "\n",
        "#2-val#\n",
        "val_x_in=[]\n",
        "val_x_mask=[]\n",
        "\n",
        "val_y=[]\n",
        "for i in range(dataset['validation'].num_rows):\n",
        "\n",
        "  temp=dataset['validation'][i]\n",
        "  sent1=temp[\"sentence1\"]\n",
        "  sent2=temp['sentence2']\n",
        "  label=temp['label']\n",
        "  tok1=tokenizer(sent1, return_tensors='pt')\n",
        "  tok2=tokenizer(sent2, return_tensors='pt')\n",
        "  #token tensor and mask\n",
        "  input1=tok1.input_ids\n",
        "  mask1=tok1.attention_mask\n",
        "\n",
        "  input2=tok2.input_ids\n",
        "  mask2=tok2.attention_mask\n",
        "\n",
        "  input=[]\n",
        "  mask=[]\n",
        "  input=torch.cat((input1,input2),1)\n",
        "  mask=torch.cat((mask1,mask2),1)\n",
        "\n",
        "  if len(input[0])<=seq_len:\n",
        "    z=torch.zeros(seq_len-len(input[0]))\n",
        "    # print(z.shape)\n",
        "\n",
        "    input=torch.cat((input[0],z))\n",
        "    mask=torch.cat((mask[0],z))\n",
        "    # print(input.shape,mask.shape)\n",
        "\n",
        "    val_x_in.append(input)\n",
        "    val_x_mask.append(mask)\n",
        "\n",
        "    val_y.append(label)\n",
        "\n",
        "\n",
        "\n",
        "val_x_in=torch.stack(val_x_in)\n",
        "val_x_in=val_x_in.to(torch.long)\n",
        "\n",
        "\n",
        "# train_x_in=torch.LongTensor(train_x_in)\n",
        "\n",
        "val_x_mask=torch.stack(val_x_mask)\n",
        "val_x_mask=val_x_mask.to(torch.long)\n",
        "\n",
        "# train_x_mask=torch.LongTensor(train_x_mask)\n",
        "val_y=torch.LongTensor(val_y)\n",
        "\n",
        "#3-test#\n",
        "test_x_in=[]\n",
        "test_x_mask=[]\n",
        "\n",
        "\n",
        "test_y=[]\n",
        "for i in range(dataset['test'].num_rows):\n",
        "\n",
        "  temp=dataset['test'][i]\n",
        "  sent1=temp[\"sentence1\"]\n",
        "  sent2=temp['sentence2']\n",
        "  label=temp['label']\n",
        "  tok1=tokenizer(sent1, return_tensors='pt')\n",
        "  tok2=tokenizer(sent2, return_tensors='pt')\n",
        "  #token tensor and mask\n",
        "  input1=tok1.input_ids\n",
        "  mask1=tok1.attention_mask\n",
        "\n",
        "  input2=tok2.input_ids\n",
        "  mask2=tok2.attention_mask\n",
        "\n",
        "  input=[]\n",
        "  mask=[]\n",
        "  input=torch.cat((input1,input2),1)\n",
        "  mask=torch.cat((mask1,mask2),1)\n",
        "\n",
        "  if len(input[0])<=seq_len:\n",
        "    z=torch.zeros(seq_len-len(input[0]))\n",
        "    # print(z.shape)\n",
        "\n",
        "    input=torch.cat((input[0],z))\n",
        "    mask=torch.cat((mask[0],z))\n",
        "    # print(input.shape,mask.shape)\n",
        "\n",
        "\n",
        "    test_x_in.append(input)\n",
        "    test_x_mask.append(mask)\n",
        "    test_y.append(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_x_in=torch.stack(test_x_in)\n",
        "test_x_in=test_x_in.to(torch.long)\n",
        "\n",
        "\n",
        "# train_x_in=torch.LongTensor(train_x_in)\n",
        "\n",
        "test_x_mask=torch.stack(test_x_mask)\n",
        "test_x_mask=test_x_mask.to(torch.long)\n",
        "\n",
        "# train_x_mask=torch.LongTensor(train_x_mask)\n",
        "\n",
        "test_y=torch.LongTensor(test_y)\n"
      ],
      "metadata": {
        "id": "3QOn-zPQgt5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_in.shape,val_x_in.shape,test_x_in.shape"
      ],
      "metadata": {
        "id": "zhE2DHCRv96L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=nn.Sequential(\n",
        "    nn.Linear(dbert_model.config.dim,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512,128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128,1),\n",
        "\n",
        ")\n",
        "\n",
        "class SentimentAnalysis(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.m1=dbert_model\n",
        "    self.m2=classifier\n",
        "  def forward(self,input,mask):\n",
        "    x=self.m1(input,mask).last_hidden_state\n",
        "    x=torch.squeeze(x, 0)\n",
        "    # print(x.shape)\n",
        "    x=torch.mean(x, 1)\n",
        "    # print(x.shape)\n",
        "    x=self.m2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model=SentimentAnalysis()"
      ],
      "metadata": {
        "id": "xO4xnLk9g-um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "metadata": {
        "id": "Q7e_fpgmg_6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_in=train_x_in.to('cuda')\n",
        "train_x_mask=train_x_mask.to('cuda')\n",
        "train_y=train_y.to('cuda')\n",
        "val_x_in=val_x_in.to('cuda')\n",
        "val_x_mask=val_x_mask.to('cuda')\n",
        "val_y=val_y.to('cuda')\n",
        "test_x_in=test_x_in.to('cuda')\n",
        "test_x_mask=test_x_mask.to('cuda')\n",
        "test_y=test_y.to('cuda')\n",
        "model=model.to('cuda')"
      ],
      "metadata": {
        "id": "WEKsK2hEhAuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "train_dataset=TensorDataset(train_x_in,train_x_mask,train_y)\n",
        "val_dataset=TensorDataset(val_x_in,val_x_mask,val_y)\n",
        "test_dataset=TensorDataset(test_x_in,test_x_mask,test_y)\n",
        "\n",
        "train = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val=DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "NiCeeZsjhF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.regression import SpearmanCorrCoef\n",
        "from torchmetrics.regression import PearsonCorrCoef\n",
        "epochs=10\n",
        "\n",
        "criterion=nn.MSELoss()\n",
        "opt=torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# spearman_train = SpearmanCorrCoef()\n",
        "# pearson_train = PearsonCorrCoef()\n",
        "\n",
        "# spearman_val= SpearmanCorrCoef()\n",
        "# pearson_val = PearsonCorrCoef()\n",
        "\n",
        "spearman_train = SpearmanCorrCoef().to('cuda')\n",
        "pearson_train = PearsonCorrCoef().to('cuda')\n",
        "\n",
        "spearman_val= SpearmanCorrCoef().to('cuda')\n",
        "pearson_val = PearsonCorrCoef().to('cuda')"
      ],
      "metadata": {
        "id": "7Y8wrk-w3Hi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "  avg_train_acc=0\n",
        "  avg_val_acc=0\n",
        "  count=0\n",
        "  avg_train_loss=0\n",
        "  avg_val_loss=0\n",
        "\n",
        "  for batch in train:\n",
        "    count=count+1\n",
        "    print(f'Epoch {epoch} Batch no.: {count}')\n",
        "    X_batch_in,X_batch_mask,label_batch = batch\n",
        "    label_batch = label_batch.float()\n",
        "\n",
        "    # print(X_batch.shape)\n",
        "    preds=model(X_batch_in,X_batch_mask)\n",
        "\n",
        "    preds=torch.squeeze(preds,1)\n",
        "\n",
        "    loss=criterion(preds,label_batch)\n",
        "    # label_batch=label_batch.to('cpu')\n",
        "    # label_batch=label_batch.float()\n",
        "    # print(preds.dtype,label_batch.dtype)\n",
        "\n",
        "\n",
        "    spearman_train.update(preds,label_batch)\n",
        "    pearson_train.update(preds,label_batch)\n",
        "\n",
        "    #avg_train_acc=avg_train_acc+acc\n",
        "    avg_train_loss=avg_train_loss+loss\n",
        "\n",
        "    # print(loss.dtype)\n",
        "    # loss=loss.float()\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val:\n",
        "      X_val_batch_in,X_val_batch_mask,label_val_batch = batch\n",
        "\n",
        "      label_val_batch=label_val_batch.float()\n",
        "\n",
        "      val_preds=model(X_val_batch_in,X_val_batch_mask)\n",
        "      val_preds=torch.squeeze(val_preds,1)\n",
        "\n",
        "      val_loss=criterion(val_preds,label_val_batch)\n",
        "\n",
        "      spearman_val.update(val_preds,label_val_batch)\n",
        "      pearson_val.update(val_preds,label_val_batch)\n",
        "      #avg_val_acc=avg_val_acc+val_acc\n",
        "      avg_val_loss=avg_val_loss+val_loss\n",
        "\n",
        "\n",
        "\n",
        "  # print(f'Epoch {epoch}: Training Loss:  Training accuracy: , Validation Loss: Validation accuracy:')\n",
        "  # if epoch%5==0:\n",
        "  print(f\"| Epoch={epoch} | SpearmanTrain={spearman_train.compute()} | PearsonTrain={pearson_train.compute()} | SpearmanVal={spearman_val.compute()}  | PearsonVal={pearson_val.compute()}  | Training Loss={avg_train_loss/len(train)} | Validation_Loss={avg_val_loss/len(val)} |\")\n",
        "  print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Test acc:\n",
        "#with torch.no_grad():\n",
        "#    for batch in test:\n",
        "#      X_test_batch_in,X_test_batch_mask,label_test_batch = batch\n",
        "\n",
        "\n",
        " #     test_preds=model(X_test_batch_in,X_test_batch_mask)\n",
        "#      test_loss=criterion(test_preds,label_test_batch)\n",
        "#      test_acc=accuracy(test_preds,label_test_batch)\n",
        "#      avg_test_acc=avg_test_acc+test_acc\n",
        "#      avg_test_loss=avg_test_loss+test_loss\n",
        "\n",
        "\n",
        "#print(f'Test loss: {avg_test_loss/len(test)} | Test acc: {avg_test_acc/len(test)}')\n",
        "\n",
        "#new_model = model.to('cpu')\n",
        "torch.save(model,'test_model_distilbert_stsb.pt')\n",
        "\n",
        "# test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to('cuda')\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test:\n",
        "#       X_test_batch_in,X_test_batch_mask,label_test_batch = batch\n",
        "\n",
        "\n",
        "#       test_preds=model(X_test_batch_in,X_test_batch_mask)\n",
        "#       test_loss=criterion(test_preds,label_test_batch)\n",
        "#       test_accuracy.update(test_preds,label_test_batch)\n",
        "#       avg_test_loss=avg_test_loss+test_loss\n",
        "\n",
        "\n",
        "# print(f'Test loss: {avg_test_loss/len(test)} | Test acc: test_accuracy.compute() * 100')\n",
        "\n"
      ],
      "metadata": {
        "id": "tFeB3_fLg2uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVUSc57wBeF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}