# nlp_course_project
Implementation of DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
## The KD code is in a different repo- 
![https://github.com/sankn123/KD_Lib]
