# NLP Course Project
Implementation of DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
- Three ipynb files for the htree datasets metnioned in the paper.
## The KD code is in a different repo- 
https://github.com/sankn123/KD_Lib
